---
title: "Sampling Distributions"
subtitle: "Sampling Exponential Random Variates"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    font_size: 20pts
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---

<style type="text/css">
  body{
  font-size: 20pt;
}
</style>

```{r eval=FALSE,include=FALSE}

# CT6 
# - ASI 2005 April Question B - 1
# - ASI 2006 April Question B - 1

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(knitr)
library(dplyr)
library(janitor)


```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent_inverse(
  #base_color = "#4169e1", #)(vvv#TREES
  #background_color = "#E6E6FA",  #TREE
  base_color = "black", #)(
  background_color =  "#ACE7F8",
  text_color="black",
  text_bold_color = "black",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "400", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```


<style> 
  /* Hide page numbers */ .
  remark-slide-number { display: none; } 
</style>

# Information Theory
## Joint Entropy Calculations
<br>
## Discrete Maths for Computer Science Students
<br><br><br><br>
### <tt>DiscreteMaths.github.io</tt>

---

### Joint Entropy
The input source to a noisy communication channel is a random variable X over three symbols $\{a,b,c\}$. 

The output from this channel is a random variable $Y$ over the same three symbols. The joint distribution of the these two random variables is as follows:

$$\begin{array}{|c|c|c|c|}
\hline
	&	x=a	&	x=b	&	x=c	\\ \hline
y=a	&	0.25	&	0	&	0.125	\\ \hline
y=b	&	0	&	0.125	&	0	\\ \hline
y=c	&	0.125	&	0.25	&	0.125	\\ \hline
\end{array}$$
### Exercises

A. Write down the marginal distributions for X and Y.

B. Compute the marginal entropies $H(X)$ and $H(Y)$

C. Compute the joint entropy $H(X,Y)$ of the two random variables.


---
### Exercise A. 

**Write down the marginal distributions for $X$ and $Y$**

$$\begin{array}{|c|c|c|c|c|}
\hline
	&	x=a	&	x=b	&	x=c	& \text{Total}\\ \hline
y=a	&	0.25	&	0	&	0.125	& 0.375 \\ \hline
y=b	&	0	&	0.125	&	0	& 0.125 \\ \hline
y=c	&	0.125	&	0.25	&	0.125	& 0.50\\ \hline
\text{Total} & 0.375 &  0.375 & 0.25 & 1.00\\ \hline
\end{array}$$

---

#### Marginal Distribution for $X$:

To find the marginal distribution of $X$, we sum the joint probabilities across all values of $Y$ for each $X$.

- $P(X = a) = P(y = a \cap x = a) + P(y = c \cap x = a)$
$$
P(X = a) = 0.25 + 0.125 = 0.375
$$

- $P(X = b) = P(y = b \cap x = b) + P(y = c \cap x = b)$
$$
P(X = b) = 0.125 + 0.25 = 0.375
$$

- $P(X = c) = P(y = a \cap x = c) + P(y = c \cap x = c)$
$$
P(X = c) = 0.125 + 0.125 = 0.25
$$

Thus, the marginal distribution for $X$ is:

$P(X) = \{P(X = a) = 0.375, \; P(X = b) = 0.375, \; P(X = c) = 0.25\}$


---

#### Marginal Distribution for $Y$:

To find the marginal distribution of $Y$, we sum the joint probabilities across all values of $X$ for each $Y$.

- $P(Y = a) = P(y = a \cap x = a) + P(y = a \cap x = c)$
$$
P(Y = a) = 0.25 + 0.125 = 0.375
$$

- $P(Y = b) = P(y = b \cap x = b)$
$$
P(Y = b) = 0.125
$$

- $P(Y = c) = P(y = c \cap x = a) + P(y = c \cap x = b) + P(y = c \cap x = c)$
$$
P(Y = c) = 0.125 + 0.25 + 0.125 = 0.5
$$

Thus, the marginal distribution for $Y$ is:

$P(Y) = \{P(Y = a) = 0.375, \; P(Y = b) = 0.125, \; P(Y = c) = 0.5\}$

---

### Exercise B. 

**Compute the marginal entropies $H(X)$ and $H(Y)$**

#### Marginal Entropy $H(X)$:
$$
H(X) = - \sum_{i} P(x_i) \log_2 P(x_i)
$$
$$
H(X) = - (0.375 \log_2 0.375 + 0.375 \log_2 0.375 + 0.25 \log_2 0.25)
$$
$$
H(X) = - (0.375 \times (-1.415) + 0.375 \times (-1.415) + 0.25 \times (-2))
$$
$$
H(X) \approx 1.561 \text{ bits}
$$

---

#### Marginal Entropy $H(Y)$:
$$
H(Y) = - \sum_{j} P(y_j) \log_2 P(y_j)
$$
$$
H(Y) = - (0.375 \log_2 0.375 + 0.125 \log_2 0.125 + 0.5 \log_2 0.5)
$$
$$
H(Y) = - (0.375 \times (-1.415) + 0.125 \times (-3) + 0.5 \times (-1))
$$
$$
H(Y) \approx 1.4056 \text{ bits}
$$

---

### Exercise C. 

**Compute the joint entropy $H(X, Y)$ of the two random variables**

The joint entropy $H(X, Y)$ is calculated as:
$$
H(X, Y) = - \sum_{i,j} P(x_i, y_j) \log_2 P(x_i, y_j)
$$

---

### Exercise C. 

Using the given joint probabilities:

$$\begin{eqnarray}
H(X, Y) &=& - (& 0.25 \log_2 0.25 & + 0 \log_2 0 &+ 0.125 \log_2 0.125 \\ 
& & + & 0 \log_2 0 & + 0.125 \log_2 0.125  &+ 0 \log_2 0 \\
& & + & 0.125 \log_2 0.125 &+ 0.25 \log_2 0.25  & +  0.125 \log_2 0.125 )\\
\end{eqnarray}$$

---

### Exercise C. 

Ignoring the zero terms:
$$\begin{eqnarray}
H(X, Y) &=& - [& 0.25 (-2) &  &+ 0.125 (-3) \\ 
& &  & & + 0.125 (-3)  &  \\
& & + & 0.125 (-3) &+ 0.25 (-2) & +  0.125 (-3) ]\\
\end{eqnarray}$$

<br>

$$
H(X, Y) = - (\; [2 \times 0.25 \times (-2)] \; +\; [4 \times 0.125 \times (-3)]  \;)
$$
(Remove "-" signs)
$$ = (2 \times 0.5) + (4 \times 0.375)$$
$$
= 2.5 \text{ bits}
$$

---

### Summary:

- Marginal distribution for $X$: $\{P(X = a) = 0.375, \; P(X = b) = 0.375, \; P(X = c) = 0.25\}$

- Marginal distribution for $Y$: $\{P(Y = a) = 0.375, \; P(Y = b) = 0.125, \; P(Y = c) = 0.5\}$

- Marginal entropy $H(X) \approx 1.561 \text{ bits}$

- Marginal entropy $H(Y) \approx 1.4056 \text{ bits}$

- Joint entropy $H(X, Y) \approx 2.5 \text{ bits}$

---

---